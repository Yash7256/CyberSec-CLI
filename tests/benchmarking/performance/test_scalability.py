"""
Scalability benchmarks for CyberSec-CLI.
Tests horizontal and vertical scaling characteristics.
"""

import asyncio
import sys
from pathlib import Path
from typing import Dict, List

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent.parent.parent))

from tests.benchmarking.framework.base_benchmark import BaseBenchmark


class ScalabilityBenchmark(BaseBenchmark):
    """
    Benchmark scalability of port scanning operations.
    
    Tests:
    - Horizontal scaling (increasing number of targets)
    - Vertical scaling (increasing ports per target)
    - Concurrent operations
    """

    def __init__(self):
        """Initialize scalability benchmark."""
        super().__init__("scalability", "tests/benchmarking/results/performance")

    async def benchmark_horizontal_scaling(
        self, target_counts: List[int] = [1, 10, 100, 1000]
    ) -> Dict:
        """
        Test horizontal scaling (increasing number of targets).
        
        Args:
            target_counts: List of target counts to test
            
        Returns:
            Dictionary with scaling results
        """
        print("Benchmarking horizontal scaling...")
        results = {}

        for count in target_counts:
            print(f"  Testing with {count} targets...")

            async def scan_multiple_targets():
                try:
                    from cybersec_cli.tools.network.port_scanner import PortScanner

                    tasks = []
                    for i in range(count):
                        # Use different IPs to avoid cache
                        target = f"127.0.0.{(i % 254) + 1}"
                        scanner = PortScanner(
                            target=target,
                            ports=[80, 443],
                            timeout=0.5,
                            max_concurrent=5,
                        )
                        tasks.append(scanner.scan())

                    await asyncio.gather(*tasks, return_exceptions=True)

                except ImportError:
                    # Mock for testing
                    await asyncio.sleep(count * 0.01)

            metrics = await self.run_with_metrics(
                scan_multiple_targets,
                operations=count,
                metadata={"target_count": count, "ports_per_target": 2},
            )

            results[f"{count}_targets"] = {
                "target_count": count,
                "duration": metrics.duration,
                "throughput": metrics.throughput,
                "memory_diff_mb": metrics.memory_diff_mb,
                "targets_per_second": count / metrics.duration if metrics.duration > 0 else 0,
            }

            print(f"    Duration: {metrics.duration:.2f}s, Throughput: {metrics.throughput:.1f} targets/sec")

        return results

    async def benchmark_vertical_scaling(
        self, port_counts: List[int] = [10, 100, 1000, 5000]
    ) -> Dict:
        """
        Test vertical scaling (increasing ports per target).
        
        Args:
            port_counts: List of port counts to test
            
        Returns:
            Dictionary with scaling results
        """
        print("\nBenchmarking vertical scaling...")
        results = {}

        for count in port_counts:
            print(f"  Testing with {count} ports...")

            async def scan_many_ports():
                try:
                    from cybersec_cli.tools.network.port_scanner import PortScanner

                    scanner = PortScanner(
                        target="127.0.0.1",
                        ports=list(range(1, count + 1)),
                        timeout=0.3,
                        max_concurrent=20,
                    )
                    await scanner.scan()

                except ImportError:
                    # Mock for testing
                    await asyncio.sleep(count * 0.0001)

            metrics = await self.run_with_metrics(
                scan_many_ports,
                operations=count,
                metadata={"port_count": count, "target": "127.0.0.1"},
            )

            results[f"{count}_ports"] = {
                "port_count": count,
                "duration": metrics.duration,
                "throughput": metrics.throughput,
                "memory_diff_mb": metrics.memory_diff_mb,
                "ports_per_second": count / metrics.duration if metrics.duration > 0 else 0,
            }

            print(f"    Duration: {metrics.duration:.2f}s, Ports/sec: {results[f'{count}_ports']['ports_per_second']:.1f}")

        return results

    async def benchmark_concurrent_operations(
        self, concurrency_levels: List[int] = [1, 5, 10, 25, 50]
    ) -> Dict:
        """
        Test concurrent scan operations.
        
        Args:
            concurrency_levels: List of concurrency levels to test
            
        Returns:
            Dictionary with concurrency results
        """
        print("\nBenchmarking concurrent operations...")
        results = {}

        for level in concurrency_levels:
            print(f"  Testing with {level} concurrent scans...")

            async def concurrent_scans():
                try:
                    from cybersec_cli.tools.network.port_scanner import PortScanner

                    tasks = []
                    for i in range(level):
                        target = f"127.0.0.{(i % 254) + 1}"
                        scanner = PortScanner(
                            target=target,
                            ports=list(range(1, 51)),  # 50 ports each
                            timeout=0.5,
                            max_concurrent=10,
                        )
                        tasks.append(scanner.scan())

                    await asyncio.gather(*tasks, return_exceptions=True)

                except ImportError:
                    # Mock for testing
                    await asyncio.sleep(level * 0.05)

            metrics = await self.run_with_metrics(
                concurrent_scans,
                operations=level,
                metadata={"concurrency_level": level, "ports_per_scan": 50},
            )

            results[f"concurrency_{level}"] = {
                "concurrency_level": level,
                "duration": metrics.duration,
                "throughput": metrics.throughput,
                "memory_diff_mb": metrics.memory_diff_mb,
                "cpu_percent": metrics.cpu_percent,
            }

            print(f"    Duration: {metrics.duration:.2f}s, CPU: {metrics.cpu_percent:.1f}%")

        return results

    async def analyze_scaling_efficiency(self, results: Dict) -> Dict:
        """
        Analyze scaling efficiency from results.
        
        Args:
            results: Results from scaling benchmarks
            
        Returns:
            Dictionary with efficiency analysis
        """
        analysis = {}

        # Check if we have horizontal scaling results
        if any("_targets" in k for k in results.keys()):
            target_results = {k: v for k, v in results.items() if "_targets" in k}
            
            # Calculate scaling factor
            baseline = list(target_results.values())[0]
            scaling_factors = []
            
            for result in list(target_results.values())[1:]:
                expected_duration = baseline["duration"] * (result["target_count"] / baseline["target_count"])
                actual_duration = result["duration"]
                efficiency = expected_duration / actual_duration if actual_duration > 0 else 0
                scaling_factors.append(efficiency)
            
            analysis["horizontal_scaling"] = {
                "mean_efficiency": sum(scaling_factors) / len(scaling_factors) if scaling_factors else 0,
                "scaling_type": "linear" if all(e > 0.8 for e in scaling_factors) else "sublinear",
            }

        return analysis

    async def run_benchmark(self) -> Dict:
        """Run all scalability benchmarks."""
        print("\n" + "=" * 60)
        print("Scalability Benchmark Suite")
        print("=" * 60 + "\n")

        results = {}

        # Horizontal scaling (Table III)
        results["horizontal"] = await self.benchmark_horizontal_scaling([1, 10, 100, 1000])

        # Vertical scaling
        results["vertical"] = await self.benchmark_vertical_scaling([100, 1000, 5000])

        # Concurrent operations
        results["concurrent"] = await self.benchmark_concurrent_operations([1, 10, 50, 100])


        # Analyze efficiency
        results["analysis"] = await self.analyze_scaling_efficiency(results["horizontal"])

        # Save results
        filepath = self.save_results("scalability_results.json")
        print(f"\nâœ“ Results saved to: {filepath}")

        # Print summary
        self.print_summary()

        return results


async def main():
    """Run the scalability benchmark suite."""
    benchmark = ScalabilityBenchmark()
    results = await benchmark.run_benchmark()

    print("\n" + "=" * 60)
    print("Scalability Benchmark Complete!")
    print("=" * 60)

    return results


if __name__ == "__main__":
    asyncio.run(main())
