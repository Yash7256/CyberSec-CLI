#!/usr/bin/env python3
"""
Quick demo script to showcase the comprehensive testing infrastructure.
Runs a subset of tests to demonstrate capabilities.
"""

import asyncio
import sys
from pathlib import Path

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))


async def main():
    """Run demo tests."""
    print("\n" + "=" * 70)
    print("CYBERSEC-CLI COMPREHENSIVE TESTING INFRASTRUCTURE - DEMO")
    print("=" * 70)
    print("\nThis demo showcases the testing infrastructure capabilities.")
    print("For full tests, use: python run_all_benchmarks.py")
    print("\n" + "=" * 70 + "\n")

    results = {}

    # 1. Framework Verification
    print("1Ô∏è‚É£  FRAMEWORK VERIFICATION")
    print("-" * 70)
    try:
        from tests.benchmarking.framework.base_benchmark import BaseBenchmark
        from tests.benchmarking.framework.metrics_collector import MetricsCollector
        from tests.benchmarking.framework.statistical_analysis import StatisticalAnalyzer
        
        print("‚úì Framework components loaded successfully")
        results["framework"] = "PASS"
    except Exception as e:
        print(f"‚úó Framework error: {e}")
        results["framework"] = "FAIL"
    
    print()

    # 2. Performance Test Sample
    print("2Ô∏è‚É£  PERFORMANCE BENCHMARK SAMPLE")
    print("-" * 70)
    try:
        from tests.benchmarking.performance.test_speed_throughput import SpeedThroughputBenchmark
        
        benchmark = SpeedThroughputBenchmark()
        print("Running single port scan benchmark (10 iterations)...")
        result = await benchmark.benchmark_single_port_scan(iterations=10)
        print(f"‚úì Mean latency: {result.get('mean_latency_ms', 0):.2f}ms")
        results["performance"] = "PASS"
    except Exception as e:
        print(f"‚úó Performance test error: {e}")
        results["performance"] = "FAIL"
    
    print()

    # 3. Scalability Test Sample
    print("3Ô∏è‚É£  SCALABILITY TEST SAMPLE")
    print("-" * 70)
    try:
        from tests.benchmarking.performance.test_scalability import ScalabilityBenchmark
        
        benchmark = ScalabilityBenchmark()
        print("Running horizontal scaling test (1, 10 targets)...")
        result = await benchmark.benchmark_horizontal_scaling([1, 10])
        print(f"‚úì Scalability test completed")
        results["scalability"] = "PASS"
    except Exception as e:
        print(f"‚úó Scalability test error: {e}")
        results["scalability"] = "FAIL"
    
    print()

    # 4. Accuracy Test Sample
    print("4Ô∏è‚É£  ACCURACY TEST SAMPLE")
    print("-" * 70)
    try:
        from tests.benchmarking.accuracy.test_port_detection import AccuracyBenchmark
        
        benchmark = AccuracyBenchmark()
        print("Running port detection accuracy test...")
        result = await benchmark.benchmark_port_detection_accuracy(
            target="127.0.0.1",
            expected_open_ports={22, 80, 443},
            port_range="1-100"
        )
        print(f"‚úì Accuracy: {result.get('accuracy', 0):.2%}")
        print(f"‚úì Precision: {result.get('precision', 0):.2%}")
        print(f"‚úì Recall: {result.get('recall', 0):.2%}")
        results["accuracy"] = "PASS"
    except Exception as e:
        print(f"‚úó Accuracy test error: {e}")
        results["accuracy"] = "FAIL"
    
    print()

    # 5. Stress Test Sample
    print("5Ô∏è‚É£  STRESS TEST SAMPLE")
    print("-" * 70)
    try:
        from tests.benchmarking.reliability.test_stress import StressBenchmark
        
        benchmark = StressBenchmark()
        print("Running memory stress test (100MB, 5s)...")
        result = await benchmark.benchmark_memory_stress(target_mb=100, duration=5)
        print(f"‚úì Memory stress test completed")
        print(f"  Peak memory: {result.get('memory_peak_mb', 0):.1f}MB")
        results["stress"] = "PASS"
    except Exception as e:
        print(f"‚úó Stress test error: {e}")
        results["stress"] = "FAIL"
    
    print()

    # Summary
    print("=" * 70)
    print("DEMO SUMMARY")
    print("=" * 70)
    
    passed = sum(1 for r in results.values() if r == "PASS")
    total = len(results)
    
    print(f"\nTests Passed: {passed}/{total} ({passed/total*100:.0f}%)")
    print("\nTest Results:")
    for test, result in results.items():
        status = "‚úì" if result == "PASS" else "‚úó"
        print(f"  {status} {test.capitalize()}: {result}")
    
    print("\n" + "=" * 70)
    print("AVAILABLE TEST SUITES")
    print("=" * 70)
    print("\nüìä Performance Benchmarking:")
    print("  ‚Ä¢ python tests/benchmarking/performance/test_speed_throughput.py")
    print("  ‚Ä¢ python tests/benchmarking/performance/test_scalability.py")
    print("  ‚Ä¢ sudo python tests/benchmarking/performance/test_network_conditions.py")
    
    print("\nüîç Comparative Analysis:")
    print("  ‚Ä¢ python tests/benchmarking/comparative/test_nmap_comparison.py")
    print("  ‚Ä¢ sudo python tests/benchmarking/comparative/test_masscan_comparison.py")
    print("  ‚Ä¢ python tests/benchmarking/comparative/test_rustscan_comparison.py")
    
    print("\nüí™ Reliability & Stability:")
    print("  ‚Ä¢ python tests/benchmarking/reliability/test_stress.py")
    print("  ‚Ä¢ python tests/benchmarking/reliability/test_endurance.py --duration=1")
    print("  ‚Ä¢ python tests/benchmarking/reliability/test_chaos.py")
    
    print("\nüéØ Accuracy & Correctness:")
    print("  ‚Ä¢ python tests/benchmarking/accuracy/test_port_detection.py")
    
    print("\nüöÄ Run All Tests:")
    print("  ‚Ä¢ python tests/benchmarking/run_all_benchmarks.py")
    print("  ‚Ä¢ python tests/benchmarking/run_all_benchmarks.py --phases performance reliability")
    
    print("\n" + "=" * 70)
    print("For more information, see:")
    print("  ‚Ä¢ tests/benchmarking/QUICKSTART.md")
    print("  ‚Ä¢ tests/benchmarking/SUMMARY.md")
    print("  ‚Ä¢ tests/benchmarking/README.md")
    print("=" * 70 + "\n")
    
    return 0 if passed == total else 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
